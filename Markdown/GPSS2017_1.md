### What is Machine Learning?

$$ \text{data} + \text{model} \rArr \text{prediction}$$

-   $\text{data}$ : observations, could be actively or passively
    acquired (meta-data).

-   $\text{model}$ : assumptions, based on previous experience (other data!
    transfer learning etc), or beliefs about the regularities of
    the universe. Inductive bias.

-   $\text{prediction}$ : an action to be taken or a categorization or a
    quality score.

### Two important Gaussian Properties
- Sum of Gaussian
  Sum of Gaussian variables is also Gaussian.
  $$ y_{i} \sim \mathcal{N}(\mu_{i},\sigma_{i}^{2})$$
  And the sum is distributed as
  $$\sum y_{i} \sim \mathcal{N}(\sum \mu_{i},\sum \sigma_{i}^{2})$$
  Aside: As sum increase, sum of non-Gaussian, finite variance variables is also Gaussian because of **central limit theorem**.
- Scaling a Gaussian
  Scaling a Gaussian leads to a Gaussian.
  $$\omega y \sim \mathcal{N}(\omega \mu, \omega^{2}\sigma^{2})$$


The ****$$\$$central limit theorem**** (CLT) establishes that, in some situations, when independent random variables are added, their properly normalized sum tends toward a normal distribution (informally a "bell curve") even if the original variables themselves are not normally distributed. 

### Prior Distribution

-   Bayesian inference requires a prior on the parameters.

-   The prior represents your belief *before* you see the data of the
    likely value of the parameters.

-   For linear regression, consider a Gaussian prior on the intercept:
    $$c \sim \mathcal{N}(0, \alpha_1)$$

### Posterior Distribution

-   Posterior distribution is found by combining the prior with
    the likelihood.

-   Posterior distribution is your belief *after* you see the data of
    the likely value of the parameters.

-   The posterior is found through **Bayesâ€™ Rule**
    $$p(c|y) = \frac{p(y|c)p(c)}{p(y)}$$
